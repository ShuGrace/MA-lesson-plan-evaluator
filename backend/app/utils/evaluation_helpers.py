# app/utils/evaluation_helpers.py
"""
Evaluation Helper Functions - Framework v3.0
è¯„ä¼°è¾…åŠ©å‡½æ•°

âœ… v3.0 Updates:
- Added support for 'lesson_design' score extraction
- Updated to handle 4 dimensions (PBL, CRMP, CP, LDQ)
- Improved score extraction patterns
"""
import re
import json
from typing import List, Dict, Any, Optional


def extract_score_from_response(response: str, score_type: str = "general") -> int:
    """
    ä»Agentå“åº”ä¸­æå–åˆ†æ•°ï¼Œå°†5åˆ†åˆ¶è½¬æ¢ä¸º100åˆ†åˆ¶
    
    âœ… Framework v3.0: Supports all 4 dimensions
    - place_based / place
    - cultural / cultural_responsiveness
    - critical / critical_pedagogy
    - design / lesson_design / lesson_design_quality  # âœ… v3.0 new
    
    Args:
        response: Agentçš„åŸå§‹å“åº”æ–‡æœ¬
        score_type: åˆ†æ•°ç±»å‹ï¼ˆç”¨äºæ—¥å¿—å’Œç‰¹å®šæ¨¡å¼åŒ¹é…ï¼‰
    
    Returns:
        int: 0-100çš„åˆ†æ•°
    
    Examples:
        >>> extract_score_from_response("Overall Score: 4.5/5", "test")
        90
        >>> extract_score_from_response("Score: 3/5.0", "test")
        60
        >>> extract_score_from_response("Converted to 100-point scale: 85/100", "test")
        85
    """
    try:
        if not response or not isinstance(response, str):
            print(f"âš ï¸ Invalid response for {score_type}")
            return 0
        
        # âœ… v3.0: æ‰©å±•çš„æ¨¡å¼åˆ—è¡¨ï¼ˆä¼˜å…ˆåŒ¹é…å·²è½¬æ¢çš„100åˆ†åˆ¶åˆ†æ•°ï¼‰
        patterns = [
            # 100-point scale patterns (highest priority)
            r'(?:overall|composite|final|total|integrated)\s*(?:score|rating)?\s*:?\s*(\d+)\s*(?:/\s*100)?',
            r'(?:convert(?:ed)?|scale|100-point)\s*(?:score|rating)?\s*:?\s*(\d+)\s*(?:/\s*100)?',
            r'\*\*(?:convert(?:ed)?|100-point)\s*(?:score|rating)?\*\*\s*:?\s*(\d+)',
            
            # Dimension-specific patterns (100-point)
            r'place[- ]?based\s+learning\s*:?\s*(\d+)\s*(?:/\s*100)?',
            r'cultural\s+responsiveness\s*(?:integrated)?\s*:?\s*(\d+)\s*(?:/\s*100)?',
            r'critical\s+pedagogy\s*:?\s*(\d+)\s*(?:/\s*100)?',
            r'lesson\s+design\s+quality\s*:?\s*(\d+)\s*(?:/\s*100)?',  # âœ… v3.0 new
            r'design\s+quality\s*:?\s*(\d+)\s*(?:/\s*100)?',  # âœ… v3.0 new
            
            # Generic 100-point patterns
            r'score\s*:?\s*(\d+)\s*(?:/\s*100)',
            r'rating\s*:?\s*(\d+)\s*(?:/\s*100)',
            
            # 5-point scale patterns (will be converted)
            r'overall.*?score\s*:?\s*(\d+\.?\d*)\s*/\s*5',
            r'score\s*:?\s*(\d+\.?\d*)\s*/\s*5',
            r'(\d+\.?\d*)\s*/\s*5\.0',
            r'(\d+\.?\d*)\s*/\s*5\s*(?:\)|$)',
            
            # Conversion calculation patterns
            r'(\d+\.?\d*)\s*/\s*5\.?0?\s*\*\s*100',
            r'\((\d+\.?\d*)\s*/\s*5\.?0?\s*\)\s*\*\s*100',
            
            # Fallback: any number followed by /100
            r'(\d+)\s*/\s*100'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, response, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                try:
                    score_str = match.group(1)
                    score = float(score_str)
                    
                    # If score looks like it's on 5-point scale, convert
                    if score <= 5.0:
                        score = (score / 5.0) * 100
                    
                    # Clamp to valid range
                    score = max(0, min(100, int(round(score))))
                    
                    # Only return if score is reasonable (> 0)
                    if score > 0:
                        return score
                        
                except (ValueError, IndexError):
                    continue
        
        # If no score found, log and return 0
        print(f"âš ï¸ Could not extract {score_type} score from response")
        print(f"   Response preview: {response[:300]}...")
        return 0
        
    except Exception as e:
        print(f"âŒ Error extracting {score_type} score: {e}")
        return 0


def extract_recommendations_from_response(response: str, max_recommendations: int = 10) -> List[str]:
    """
    ä»Agentå“åº”ä¸­æå–æ¨èå»ºè®®
    
    Args:
        response: Agentçš„åŸå§‹å“åº”æ–‡æœ¬
        max_recommendations: æœ€å¤šè¿”å›çš„æ¨èæ•°é‡
    
    Returns:
        List[str]: æ¨èå»ºè®®åˆ—è¡¨
    
    Examples:
        >>> text = "Recommendations:\\n- Add local examples\\n- Include Te Reo MÄori"
        >>> extract_recommendations_from_response(text)
        ['Add local examples', 'Include Te Reo MÄori']
    """
    try:
        if not response or not isinstance(response, str):
            return []
        
        recommendations = []
        
        # Pattern 1: æŸ¥æ‰¾ "Recommendations" æˆ– "Suggestions" éƒ¨åˆ†
        section_patterns = [
            r'recommendations?\s*(?:for\s+improvement)?:?\s*\n((?:[-â€¢*\d].*\n?)+)',
            r'suggestions?\s*(?:for\s+improvement)?:?\s*\n((?:[-â€¢*\d].*\n?)+)',
            r'improvements?:?\s*\n((?:[-â€¢*\d].*\n?)+)',
            r'areas?\s+for\s+improvement:?\s*\n((?:[-â€¢*\d].*\n?)+)',
            r'priority\s+recommendations?:?\s*\n((?:[-â€¢*\d].*\n?)+)',
        ]
        
        for pattern in section_patterns:
            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL | re.MULTILINE)
            if match:
                recs_text = match.group(1)
                lines = recs_text.strip().split('\n')
                
                for line in lines:
                    line = line.strip()
                    # æ¸…ç†åˆ—è¡¨æ ‡è®° (-, *, 1., 2., etc.)
                    line = re.sub(r'^[\-\*â€¢]+\s*', '', line)
                    line = re.sub(r'^\d+[\.\)]\s*', '', line)
                    
                    # è¿‡æ»¤å¤ªçŸ­çš„è¡Œå’Œç©ºè¡Œ
                    if line and len(line) > 15:
                        recommendations.append(line)
                
                # å¦‚æœæ‰¾åˆ°äº†æ¨èï¼Œå°±åœæ­¢æœç´¢å…¶ä»–æ¨¡å¼
                if recommendations:
                    break
        
        # Pattern 2: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•åŒ¹é…å•ç‹¬çš„åˆ—è¡¨é¡¹
        if not recommendations:
            list_item_pattern = r'^\s*[-â€¢*]\s+(.+?)(?=\n|$)'
            matches = re.finditer(list_item_pattern, response, re.MULTILINE)
            
            for match in matches:
                rec = match.group(1).strip()
                if rec and len(rec) > 15:
                    recommendations.append(rec)
        
        # å»é‡å’Œé™åˆ¶æ•°é‡
        seen = set()
        unique_recommendations = []
        
        for rec in recommendations:
            rec_lower = rec.lower()
            if rec_lower not in seen:
                seen.add(rec_lower)
                unique_recommendations.append(rec)
                
                if len(unique_recommendations) >= max_recommendations:
                    break
        
        return unique_recommendations
        
    except Exception as e:
        print(f"âŒ Error extracting recommendations: {e}")
        return []


def parse_json_response(response_text: str, attempt: int = 0) -> Dict[str, Any]:
    """
    è§£æLLMè¿”å›çš„JSONå“åº”ï¼Œè‡ªåŠ¨å¤„ç†markdownä»£ç å—å’Œå¸¸è§æ ¼å¼é—®é¢˜
    
    Args:
        response_text: åŸå§‹å“åº”æ–‡æœ¬
        attempt: é€’å½’å°è¯•æ¬¡æ•°
    
    Returns:
        dict: è§£æåçš„JSONå¯¹è±¡ï¼Œè§£æå¤±è´¥è¿”å›ç©ºå­—å…¸
    
    Examples:
        >>> parse_json_response('```json\\n{"key": "value"}\\n```')
        {'key': 'value'}
        >>> parse_json_response('{"key": "value"}')
        {'key': 'value'}
    """
    try:
        if not response_text:
            return {}
        
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        cleaned = response_text.strip()
        
        # ç§»é™¤ ```json å¼€å¤´
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        # ç§»é™¤ ``` å¼€å¤´
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        
        # ç§»é™¤ ``` ç»“å°¾
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        
        cleaned = cleaned.strip()
        
        # å°è¯•è§£æ
        parsed = json.loads(cleaned)
        
        # éªŒè¯è¿”å›çš„æ˜¯å­—å…¸
        if not isinstance(parsed, dict):
            print(f"âš ï¸ Parsed JSON is not a dict: {type(parsed)}")
            return {}
        
        return parsed
        
    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSON parse error (attempt {attempt}): {e}")
        
        # å°è¯•æå–èŠ±æ‹¬å·ä¹‹é—´çš„å†…å®¹ï¼ˆé€’å½’ï¼‰
        if attempt < 2:
            match = re.search(r'\{.*\}', response_text, re.DOTALL)
            if match:
                return parse_json_response(match.group(0), attempt + 1)
        
        print(f"âŒ Failed to parse JSON after {attempt + 1} attempts")
        print(f"   Response preview: {response_text[:200]}...")
        return {}
    
    except Exception as e:
        print(f"âŒ Unexpected error parsing JSON: {e}")
        return {}


def merge_and_deduplicate_recommendations(
    recommendations_lists: List[List[str]], 
    max_total: int = 12  # âœ… v3.0: increased from 10 to 12 for 4 agents
) -> List[str]:
    """
    åˆå¹¶å¤šä¸ªæ¨èåˆ—è¡¨å¹¶å»é‡
    
    âœ… Framework v3.0: Increased default max to 12 (3 per agent Ã— 4 agents)
    
    Args:
        recommendations_lists: å¤šä¸ªæ¨èåˆ—è¡¨ï¼ˆæ¥è‡ªä¸åŒAgentï¼‰
        max_total: æœ€å¤šè¿”å›çš„æ€»æ¨èæ•°
    
    Returns:
        List[str]: åˆå¹¶å»é‡åçš„æ¨èåˆ—è¡¨
    
    Examples:
        >>> lists = [['Add local context'], ['Add local context', 'Use Te Reo']]
        >>> merge_and_deduplicate_recommendations(lists)
        ['Add local context', 'Use Te Reo']
    """
    # åˆå¹¶æ‰€æœ‰æ¨è
    all_recommendations = []
    for recs in recommendations_lists:
        if isinstance(recs, list):
            all_recommendations.extend(recs)
    
    # å»é‡ï¼ˆä¿æŒé¡ºåºï¼ŒåŸºäºæ ‡å‡†åŒ–çš„æ–‡æœ¬æ¯”è¾ƒï¼‰
    seen = set()
    unique_recommendations = []
    
    for rec in all_recommendations:
        if not isinstance(rec, str):
            continue
        
        rec = rec.strip()
        
        # æ ‡å‡†åŒ–æ¯”è¾ƒï¼ˆå°å†™ï¼Œå»é™¤å¤šä½™ç©ºæ ¼ï¼‰
        normalized = ' '.join(rec.lower().split())
        
        # Check similarity with existing recommendations
        is_duplicate = False
        for seen_rec in seen:
            # å¦‚æœæ–°æ¨èæ˜¯æ—§æ¨èçš„å­ä¸²ï¼Œæˆ–è€…æ—§æ¨èæ˜¯æ–°æ¨èçš„å­ä¸²ï¼Œè§†ä¸ºé‡å¤
            if normalized in seen_rec or seen_rec in normalized:
                is_duplicate = True
                break
        
        if not is_duplicate and len(rec) > 15:
            seen.add(normalized)
            unique_recommendations.append(rec)
            
            if len(unique_recommendations) >= max_total:
                break
    
    return unique_recommendations


def calculate_weighted_score(
    scores: Dict[str, int], 
    weights: Dict[str, float]
) -> int:
    """
    è®¡ç®—åŠ æƒå¹³å‡åˆ†
    
    âœ… Framework v3.0: Supports 4 dimensions with dynamic weighting
    
    Args:
        scores: å„ç»´åº¦çš„åˆ†æ•° (0-100)
        weights: å„ç»´åº¦çš„æƒé‡ (æ€»å’Œåº”ä¸º1.0)
    
    Returns:
        int: åŠ æƒå¹³å‡åˆ† (0-100)
    
    Examples:
        >>> scores = {'place_based_learning': 80, 'cultural_responsiveness_integrated': 70}
        >>> weights = {'place_based_learning': 0.25, 'cultural_responsiveness_integrated': 0.35}
        >>> calculate_weighted_score(scores, weights)
        # Returns normalized weighted average
    """
    try:
        if not scores or not weights:
            return 0
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for dimension, score in scores.items():
            weight = weights.get(dimension, 0.0)
            
            # Only include dimensions with positive score and weight
            if weight > 0 and score > 0:
                weighted_sum += score * weight
                total_weight += weight
        
        # å½’ä¸€åŒ–ï¼ˆé˜²æ­¢æƒé‡æ€»å’Œä¸ä¸º1.0ï¼Œä¾‹å¦‚æŸäº›APIè¢«ç¦ç”¨ï¼‰
        if total_weight > 0:
            # If total weight is not 1.0, normalize it
            if abs(total_weight - 1.0) > 0.01:
                normalized_score = weighted_sum / total_weight
            else:
                normalized_score = weighted_sum
        else:
            # If no valid weights, return simple average
            if scores:
                normalized_score = sum(scores.values()) / len(scores)
            else:
                normalized_score = 0.0
        
        return max(0, min(100, int(round(normalized_score))))
        
    except Exception as e:
        print(f"âŒ Error calculating weighted score: {e}")
        return 0


def format_agent_response(
    agent_name: str,
    role: str,
    response_text: str,
    score: int,
    execution_time: float,
    dimension: Optional[str] = None,  # âœ… v3.0: singular dimension
    model: Optional[str] = None  # âœ… v3.0: add model info
) -> Dict[str, Any]:
    """
    æ ¼å¼åŒ–Agentå“åº”ä¸ºæ ‡å‡†ç»“æ„
    
    âœ… Framework v3.0: Updated to use singular 'dimension' and add 'model'
    
    Args:
        agent_name: Agentåç§° ('DeepSeek', 'Claude', 'GPT-Critical', 'GPT-Design')
        role: Agentè§’è‰²
        response_text: åŸå§‹å“åº”
        score: è¯„ä¼°åˆ†æ•° (0-100)
        execution_time: æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
        dimension: è¯„ä¼°çš„ç»´åº¦ (singular, not list)
        model: æ¨¡å‹åç§° (e.g., 'gpt-4o', 'claude-sonnet-4-20250514')
    
    Returns:
        dict: æ ¼å¼åŒ–çš„å“åº”å¯¹è±¡
    """
    response_obj = {
        "agent": agent_name,
        "role": role,
        "dimension": dimension,  # âœ… v3.0: singular
        "response": response_text,
        "score": score,
        "time": round(execution_time, 2),
        "response_length": len(response_text),
    }
    
    # âœ… v3.0: Add model if provided
    if model:
        response_obj["model"] = model
    
    return response_obj


def validate_framework_scores(scores: Dict[str, int]) -> bool:
    """
    éªŒè¯åˆ†æ•°æ˜¯å¦ç¬¦åˆ Framework v3.0 çš„ç»“æ„
    
    âœ… v3.0: Check for 4 required dimensions
    
    Args:
        scores: åˆ†æ•°å­—å…¸
    
    Returns:
        bool: æ˜¯å¦æœ‰æ•ˆ
    """
    # âœ… v3.0: Required dimensions (at least 2 should be present)
    v3_dimensions = {
        'place_based_learning',
        'cultural_responsiveness_integrated',
        'critical_pedagogy',
        'lesson_design_quality'
    }
    
    if not isinstance(scores, dict):
        return False
    
    # Count how many v3.0 dimensions are present
    present_dimensions = set(scores.keys()) & v3_dimensions
    
    # At least 2 dimensions should have valid scores
    valid_scores = sum(1 for dim in present_dimensions if scores.get(dim, 0) > 0)
    
    return valid_scores >= 2


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("="*60)
    print("Testing Evaluation Helper Functions - Framework v3.0")
    print("="*60)
    
    # æµ‹è¯•åˆ†æ•°æå–
    print("\n1. Testing score extraction (v3.0):")
    test_responses = [
        "Overall Score: 4.5/5",
        "Converted to 100-point scale: 85/100",
        "Lesson Design Quality: 78/100",  # âœ… v3.0 new
        "Score: 3/5.0",
        "Cultural Responsiveness (Integrated): 68/100",  # âœ… v3.0
    ]
    
    for resp in test_responses:
        score = extract_score_from_response(resp, "test")
        print(f"   '{resp}' -> {score}/100")
    
    # æµ‹è¯•æ¨èæå–
    print("\n2. Testing recommendation extraction:")
    test_rec = """
    Strengths: Good local integration.
    
    Recommendations:
    - Add more specific local examples with named places
    - Include Te Reo MÄori vocabulary throughout
    - Strengthen community partnerships with local iwi
    - Add explicit assessment rubrics
    
    Overall, this is a solid lesson plan with room for growth.
    """
    
    recs = extract_recommendations_from_response(test_rec)
    print(f"   Found {len(recs)} recommendations:")
    for i, rec in enumerate(recs, 1):
        print(f"   {i}. {rec}")
    
    # æµ‹è¯•JSONè§£æ
    print("\n3. Testing JSON parsing:")
    test_json = '```json\n{"key": "value", "number": 42, "array": [1, 2, 3]}\n```'
    parsed = parse_json_response(test_json)
    print(f"   Parsed: {parsed}")
    
    # æµ‹è¯•åŠ æƒåˆ†æ•° (v3.0)
    print("\n4. Testing weighted score calculation (v3.0):")
    scores = {
        "place_based_learning": 72,
        "cultural_responsiveness_integrated": 68,
        "critical_pedagogy": 75,
        "lesson_design_quality": 78
    }
    weights = {
        "place_based_learning": 0.25,
        "cultural_responsiveness_integrated": 0.35,
        "critical_pedagogy": 0.25,
        "lesson_design_quality": 0.15
    }
    weighted = calculate_weighted_score(scores, weights)
    print(f"   Scores: {scores}")
    print(f"   Weights: {weights}")
    print(f"   Weighted average: {weighted}/100")
    
    # æµ‹è¯•æ¨èå»é‡
    print("\n5. Testing recommendation deduplication:")
    lists = [
        ["Add local context", "Use Te Reo MÄori"],
        ["Add local context integration", "Include iwi partnerships"],
        ["Use Te Reo MÄori vocabulary", "Add assessment rubrics"]
    ]
    merged = merge_and_deduplicate_recommendations(lists, max_total=5)
    print(f"   Original lists: {sum(len(l) for l in lists)} items")
    print(f"   After deduplication: {len(merged)} items")
    for i, rec in enumerate(merged, 1):
        print(f"   {i}. {rec}")
    
    # æµ‹è¯•æ¡†æ¶éªŒè¯ (v3.0)
    print("\n6. Testing framework v3.0 validation:")
    valid_scores = {
        "place_based_learning": 72,
        "cultural_responsiveness_integrated": 68,
        "critical_pedagogy": 75
    }
    invalid_scores = {
        "old_dimension": 50
    }
    print(f"   Valid v3.0 scores: {validate_framework_scores(valid_scores)}")
    print(f"   Invalid scores: {validate_framework_scores(invalid_scores)}")
    
    print("\n" + "="*60)
    print("âœ… All tests completed!")
    print("="*60)


    # åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ è¿™ä¸¤ä¸ªå‡½æ•°

def extract_strengths_from_response(response: str, max_strengths: int = 10) -> List[str]:
    """
    ä» Agent å“åº”ä¸­æå– Strengths (ä¼˜ç‚¹)
    
    âœ… æ”¯æŒä¸¤ç§æ ¼å¼:
    1. COMPREHENSIVE STRENGTHS SUMMARY (ä¼˜å…ˆ)
    2. æ¯ä¸ª Indicator å†…çš„ Strengths (å¤‡é€‰)
    
    Args:
        response: Agent çš„åŸå§‹å“åº”æ–‡æœ¬
        max_strengths: æœ€å¤šè¿”å›çš„ä¼˜ç‚¹æ•°é‡
    
    Returns:
        List[str]: ä¼˜ç‚¹åˆ—è¡¨
    """
    try:
        if not response or not isinstance(response, str):
            return []
        
        strengths = []
        
        # ========== æ–¹æ³• 1: æå– COMPREHENSIVE STRENGTHS SUMMARY ==========
        comprehensive_patterns = [
            r'\*\*COMPREHENSIVE\s+STRENGTHS\s+SUMMARY:?\*\*\s*(.*?)(?=\n\*\*COMPREHENSIVE\s+AREAS|\n\*\*PRIORITY|\n\*\*TRANSFORMATIVE|\n---|\Z)',
            r'(?:comprehensive\s+)?strengths?(?:\s+summary)?:?\s*\n((?:[âœ…\-â€¢*\d].*\n?){2,})',
        ]
        
        for pattern in comprehensive_patterns:
            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL | re.MULTILINE)
            if match:
                strengths_text = match.group(1)
                lines = strengths_text.strip().split('\n')
                
                for line in lines:
                    line = line.strip()
                    # æ¸…ç†åˆ—è¡¨æ ‡è®°å’Œ emoji
                    line = re.sub(r'^[âœ…\-\*â€¢]+\s*', '', line)
                    line = re.sub(r'^\d+[\.\)]\s*', '', line)
                    line = re.sub(r'\*\*', '', line)
                    line = line.strip()
                    
                    # è¿‡æ»¤å¤ªçŸ­çš„è¡Œå’ŒåŒ…å«å…¶ä»–æ ‡é¢˜çš„è¡Œ
                    if (line and 
                        len(line) > 20 and 
                        not re.match(r'^(areas?|recommendations?|gaps?|weaknesses?|provide|write)[\s:]', line, re.IGNORECASE) and
                        not line.startswith('[') and  # å¿½ç•¥ [Strength 1: ...]
                        not line.startswith('Provide')):
                        strengths.append(line)
                
                if strengths:
                    print(f"   âœ… Found {len(strengths)} strengths from COMPREHENSIVE SUMMARY")
                    break
        
        # ========== æ–¹æ³• 2: å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ€»ç»“ï¼Œæå–æ¯ä¸ª Indicator çš„ Strengths ==========
        if not strengths:
            indicator_pattern = r'\*\*Strengths:?\*\*\s*(.*?)(?=\n\*\*Areas\s+for\s+Improvement:?|\n\*\*Recommendations?:?|\n\*\*INDICATOR|\n---|\Z)'
            indicator_matches = re.findall(indicator_pattern, response, re.DOTALL | re.IGNORECASE)
            
            for match in indicator_matches:
                lines = match.strip().split('\n')
                for line in lines:
                    line = line.strip()
                    # æ¸…ç†åˆ—è¡¨æ ‡è®°
                    line = re.sub(r'^[-â€¢*]+\s*', '', line)
                    line = re.sub(r'^\d+[\.\)]\s*', '', line)
                    line = re.sub(r'\*\*', '', line)
                    line = line.strip()
                    
                    # è¿‡æ»¤å¤ªçŸ­çš„è¡Œ
                    if line and len(line) > 20:
                        strengths.append(line)
            
            if strengths:
                print(f"   âœ… Found {len(strengths)} strengths from individual indicators")
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        seen = set()
        unique_strengths = []
        
        for strength in strengths:
            strength_lower = strength.lower()
            if strength_lower not in seen and len(strength) > 20:
                seen.add(strength_lower)
                unique_strengths.append(strength)
                
                if len(unique_strengths) >= max_strengths:
                    break
        
        return unique_strengths
        
    except Exception as e:
        print(f"âŒ Error extracting strengths: {e}")
        return []


def extract_areas_for_improvement_from_response(response: str, max_areas: int = 10) -> List[str]:
    """
    ä» Agent å“åº”ä¸­æå– Areas for Improvement (éœ€æ”¹è¿›çš„é¢†åŸŸ)
    
    âœ… æ”¯æŒä¸¤ç§æ ¼å¼:
    1. COMPREHENSIVE AREAS FOR IMPROVEMENT (ä¼˜å…ˆ)
    2. æ¯ä¸ª Indicator å†…çš„ Areas for Improvement (å¤‡é€‰)
    
    æ³¨æ„: è¿™ä¸ Recommendations ä¸åŒ
    - Areas: æŒ‡å‡ºé—®é¢˜/ç¼ºé™·/å·®è·
    - Recommendations: æä¾›è§£å†³æ–¹æ¡ˆ
    
    Args:
        response: Agent çš„åŸå§‹å“åº”æ–‡æœ¬
        max_areas: æœ€å¤šè¿”å›çš„æ•°é‡
    
    Returns:
        List[str]: éœ€æ”¹è¿›é¢†åŸŸåˆ—è¡¨
    """
    try:
        if not response or not isinstance(response, str):
            return []
        
        areas = []
        
        # ========== æ–¹æ³• 1: æå– COMPREHENSIVE AREAS FOR IMPROVEMENT ==========
        comprehensive_patterns = [
            r'\*\*COMPREHENSIVE\s+AREAS\s+FOR\s+IMPROVEMENT:?\*\*\s*(.*?)(?=\n\*\*PRIORITY|\n\*\*TRANSFORMATIVE|\n---|\Z)',
            r'(?:comprehensive\s+)?areas?\s+for\s+improvement:?\s*\n((?:[ğŸ”§\-â€¢*\d].*\n?){2,})',
        ]
        
        for pattern in comprehensive_patterns:
            match = re.search(pattern, response, re.IGNORECASE | re.DOTALL | re.MULTILINE)
            if match:
                areas_text = match.group(1)
                lines = areas_text.strip().split('\n')
                
                for line in lines:
                    line = line.strip()
                    # æ¸…ç†åˆ—è¡¨æ ‡è®°ã€emoji å’Œè­¦å‘Šç¬¦å·
                    line = re.sub(r'^[ğŸ”§âš ï¸ğŸš©âŒ\-\*â€¢]+\s*', '', line)
                    line = re.sub(r'^\d+[\.\)]\s*', '', line)
                    line = re.sub(r'^\*\*(MISSING|CRITICAL|LIMITED|WEAK)\*\*:?\s*', '', line, flags=re.IGNORECASE)
                    line = re.sub(r'\*\*', '', line)
                    line = line.strip()
                    
                    # è¿‡æ»¤å¤ªçŸ­çš„è¡Œå’ŒåŒ…å«å…¶ä»–æ ‡é¢˜çš„è¡Œ
                    if (line and 
                        len(line) > 20 and 
                        not re.match(r'^(strengths?|recommendations?|priority|provide|write)[\s:]', line, re.IGNORECASE) and
                        not line.startswith('[') and
                        not line.startswith('Provide')):
                        areas.append(line)
                
                if areas:
                    print(f"   ğŸ”§ Found {len(areas)} areas from COMPREHENSIVE SUMMARY")
                    break
        
        # ========== æ–¹æ³• 2: å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ€»ç»“ï¼Œæå–æ¯ä¸ª Indicator çš„ Areas ==========
        if not areas:
            indicator_pattern = r'\*\*Areas\s+for\s+Improvement:?\*\*\s*(.*?)(?=\n\*\*Recommendations?:?|\n\*\*INDICATOR|\n---|\Z)'
            indicator_matches = re.findall(indicator_pattern, response, re.DOTALL | re.IGNORECASE)
            
            for match in indicator_matches:
                lines = match.strip().split('\n')
                for line in lines:
                    line = line.strip()
                    # æ¸…ç†åˆ—è¡¨æ ‡è®°
                    line = re.sub(r'^[-â€¢*]+\s*', '', line)
                    line = re.sub(r'^\d+[\.\)]\s*', '', line)
                    line = re.sub(r'\*\*', '', line)
                    line = line.strip()
                    
                    # è¿‡æ»¤å¤ªçŸ­çš„è¡Œ
                    if line and len(line) > 20:
                        areas.append(line)
            
            if areas:
                print(f"   ğŸ”§ Found {len(areas)} areas from individual indicators")
        
        # å»é‡
        seen = set()
        unique_areas = []
        
        for area in areas:
            area_lower = area.lower()
            if area_lower not in seen and len(area) > 20:
                seen.add(area_lower)
                unique_areas.append(area)
                
                if len(unique_areas) >= max_areas:
                    break
        
        return unique_areas
        
    except Exception as e:
        print(f"âŒ Error extracting areas for improvement: {e}")
        return []
    
